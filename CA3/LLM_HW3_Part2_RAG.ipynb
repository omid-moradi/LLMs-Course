{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7wp7OPc9Kcc"
      },
      "source": [
        "# **CA3-Part2, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA3-Part2_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks under each `Completion` section.\n",
        "\n",
        "- For text-based answers, you should replace the text that says `WRITE YOUR ANSWER HERE` with your actual answer, or you can look for `Report` and `Question` blocks.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email or Telegram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zivgc23P8r0f"
      },
      "source": [
        "# RAG (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7koJfgPBRlNz"
      },
      "source": [
        "If you have any further questions or concerns, contact the TA via email (pouya.sadeghi@ut.ac.ir) or telegram."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNHplrzc8r0f"
      },
      "source": [
        "## Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX-Lm3M58r0f",
        "outputId": "73b6af68-e420-4d55-ed89-a5c4990dba4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain langchain_community langchain_huggingface huggingface_hub\n",
        "!pip install -q sentence_transformers tiktoken lark datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MhNsfsNVAVW",
        "outputId": "e75b7876-eda5-4556-bce4-b00bfc452973"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jul  9 12:16:30 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# To determine your system's CUDA version, run the following command:\n",
        "!nvidia-smi\n",
        "\n",
        "# Based on your CUDA version, install the appropriate FAISS-GPU package:\n",
        "\n",
        "# For CUDA 12.x:\n",
        "# !pip install -q faiss-gpu-cu12\n",
        "\n",
        "# For CUDA 11.x:\n",
        "# !pip install faiss-gpu-cu11\n",
        "\n",
        "# If you prefer the CPU-only version of FAISS:\n",
        "!pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuwfOPincl-1"
      },
      "source": [
        "## 1. An Overview of Information Retrieval (IR) and RAG (2 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHWLyGsKc59P"
      },
      "source": [
        "- **Information Retrieval (IR)**: The process of obtaining information system resources relevant to a specific information need from a collection of those resources. Each IR system consists of a collection of documents, a set of queries, and a retrieval function that ranks the documents based on their relevance to the query.\n",
        "- **Retrieval-Augmented Generation (RAG)**: A model that combines the strengths of retrieval-based and generation-based approaches. It retrieves relevant documents from a large corpus and uses them to generate a response to a query. RAG is particularly useful for tasks where the answer is not explicitly present in the training data but can be inferred from related documents.\n",
        "- **RAG Architecture**: The RAG architecture consists of two main components:\n",
        "  - **Retriever**: This component retrieves relevant documents from a large corpus based on the input query. It can be implemented using various retrieval methods, such as BM25 or dense retrieval.\n",
        "  - **Generator**: This component generates a response based on the retrieved documents and the input query. It can be implemented using transformer-based models.\n",
        "  \n",
        "In this computer assignment, you will implement a RAG pipeline using the LangChain framework. You will use two different retrievers: TF-IDF and dense retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0wV_2SCcl-2"
      },
      "source": [
        "#### Question 1: (2 points)\n",
        "1. Why do we need to use RAG?\n",
        "2. What is LangChain and how does it help in building RAG pipelines?\n",
        "<!-- 2. What are the advantages of RAG over generation methods? -->\n",
        "<!-- 3. What is the difference between dense and sparse retrievers? -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMZ64sjL9Kci"
      },
      "source": [
        "1.  **Why do we need to use RAG?**\n",
        "    We need to use RAG primarily to address the inherent limitations of standalone Large Language Models (LLMs). LLMs are trained on a fixed dataset, meaning their knowledge is static and quickly becomes outdated concerning recent events or niche information. They also suffer from a tendency to \"hallucinate,\" generating plausible but factually incorrect information. Furthermore, LLMs have a limited context window, preventing them from directly processing an entire large knowledge base. RAG overcomes these issues by allowing LLMs to retrieve relevant, up-to-date information from an external corpus and use it to ground their responses, thereby reducing hallucinations and providing more accurate and verifiable answers.\n",
        "\n",
        "2.  **What is LangChain and how does it help in building RAG pipelines?**\n",
        "    LangChain is a powerful framework designed for developing applications powered by language models. It offers a modular set of components that facilitate working with LLMs, including connecting them to external data sources, integrating various tools, and constructing complex \"chains\" and \"agents\". For building RAG pipelines, LangChain provides the structural backbone to seamlessly integrate the \"Retriever\" and \"Generator\" components. It simplifies the process of fetching relevant documents, passing them as context to the LLM, and managing the overall flow from user query to model response, making the development of sophisticated RAG systems more streamlined and efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiB9LEjb8r0f"
      },
      "source": [
        "## 2. An Overview of LangChain (12 points + 2)\n",
        "\n",
        "In this overview, we will provide a step-by-step guide on how to construct a basic application using LangChain. To learn more about this framework, check its [tutorial](https://python.langchain.com/docs/tutorials/) which is available for different releases!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPaEdN5E8r0g"
      },
      "source": [
        "### 2.1 Lets load our model (4 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRahQhR_cl-2"
      },
      "source": [
        "#### Question 2: (2 points)\n",
        "\n",
        "1. Explain how different parameters, such as `temperature`, `max_length`, `top_p`, `top_k`, and `repetition_penalty`, affect the generation process in a language model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdHOM7QT9Kci"
      },
      "source": [
        "#### Question 2: (2 points)\n",
        "1. Explain how different parameters, such as `temperature`, `max_length`, `top_p`, `top_k`, and `repetition_penalty`, affect the generation process in a language model.\n",
        "\n",
        "| Parameter          | Explanation of Effect on Generation Process                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
        "| :----------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| `temperature`      | Controls the randomness or \"creativity\" of the output. It scales the logits (raw prediction scores) before applying softmax, effectively smoothing (higher temperature, e.g., 0.7+) or sharpening (lower temperature, e.g., 0.2-) the probability distribution over the next tokens. Higher temperatures lead to more diverse, surprising, and potentially less coherent text, while lower temperatures result in more predictable, conservative, and often repetitive output, reducing the likelihood of hallucinations. A temperature of 0.0 results in greedy decoding, always picking the most probable token.                                                                                                                                                                                                                              |\n",
        "| `max_new_tokens`   | Defines the maximum number of new tokens the model is allowed to generate in its response. This parameter directly controls the length of the generated output. It is crucial for managing computational resources, preventing excessively long or irrelevant responses, and ensuring the output fits within desired constraints (e.g., character limits for a UI, or specific response brevity requirements for a task).                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
        "| `top_p` (Nucleus Sampling) | Filters the set of possible next tokens. Instead of considering all tokens, the model samples only from the smallest set of tokens whose cumulative probability exceeds `p`. For example, if `top_p=0.9`, the model considers only the most probable tokens that collectively make up 90% of the probability distribution. This dynamically adjusts the number of tokens considered based on the probability distribution, allowing for diverse generation while avoiding very low-probability (and often nonsensical) tokens. It offers a more adaptive form of sampling compared to `top_k`.                                                                                                                                                                                                                                   |\n",
        "| `top_k`            | Filters the set of possible next tokens by considering only the `k` most likely tokens. For example, if `top_k=50`, the model will only sample from the 50 tokens with the highest probabilities, regardless of their cumulative probability sum. This helps to prevent the model from generating rare or irrelevant words, leading to more coherent and on-topic text. However, it is less adaptive than `top_p` because the number of tokens considered (`k`) remains constant even if the distribution is very sharp (many high-probability tokens) or very flat (all tokens have similar low probabilities).                                                                                                                                                                                                                            |\n",
        "| `repetition_penalty` | Discourages the model from repeating words, phrases, or even longer sequences that have already appeared in the prompt or in the generated text. It works by reducing the probability scores of tokens that have already been generated, making it less likely for the model to select them again. This parameter is particularly useful for improving the fluency and diversity of the generated text, preventing monotonous or circular outputs, and ensuring the model explores a wider range of vocabulary and ideas.                                                                                                                                                                                                                                                                                                                                         |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKqpgVSmcl-2"
      },
      "source": [
        "#### Completion 1: (2 points)\n",
        "\n",
        "Load the `microsoft/Phi-4-mini-instruct` model and its tokenizer, and create a `text-generation` pipeline. Use the LangChain framework to integrate the model into your application. You should configure the pipeline with appropriate parameters, such as *max_new_tokens*, *temperature*, *top_p*, *top_k*, and *repetition_penalty*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85,
          "referenced_widgets": [
            "46220383625f491c9a0e68523c613a1a",
            "e1fee0b757ed4b10ba4d5178824cb997",
            "9ba8a4c4164c4483acad0aa291dcaf08",
            "a8fbc367d7e14f3c9b5b3b65cd30f9fb",
            "15c0b1991d6d4b1cbb03cf89a5b0c9ad",
            "b1f1e54d3e414b2ba0907e0a1bb77fc3",
            "ec5066c7069b43558d3275c333be99bf",
            "6a4999c6de92454094db1889e880ea1c",
            "ea66e2c0f2d24c569a0564dd039fe9ef",
            "c31195c5b5ed4e9783d33a7ba7f2a59b",
            "f539d2fc979747e08538a59215d0b3cc"
          ]
        },
        "id": "64-XOX3M8r0g",
        "outputId": "2320a28a-3a8c-4e18-9e7b-0bd8c447fe75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "46220383625f491c9a0e68523c613a1a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain_huggingface.llms import HuggingFacePipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "\n",
        "# Load the model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "generation_args= {\n",
        "    'max_new_tokens':512,\n",
        "    'temperature':0.3, # Lowered temperature for less creative/more structured output\n",
        "    'top_p':0.8, # Lowered top_p for less diverse/more focused output\n",
        "    'top_k':50,\n",
        "    'repetition_penalty':1.1\n",
        "}\n",
        "\n",
        "# Create the pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    **generation_args\n",
        ")\n",
        "\n",
        "# Load the pipeline into LangChain\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAxNYgBGcl-3",
        "outputId": "bb6408db-47b9-46c6-89c8-006db6579912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Who is the president of the United States? The President of the United States serves as both head of state and government. As of my last update in October 2023, Joe Biden holds this office.\n",
            "\n",
            "Is there anything else you'd like to know about him or another subject?\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = llm.invoke(\"Who is the president of the United States?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joCaX8tY8r0g"
      },
      "source": [
        "### 2.2 Simple Chain (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdkMm3Mz8r0h"
      },
      "source": [
        "#### Completion 2: (2 points)\n",
        "\n",
        "Complete the next cell to create a simple chain that takes the name of a football (soccer) player as input and outputs some information about that person. To do so:\n",
        "\n",
        "1. Use the `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes to construct a conversational prompt.\n",
        "2. Use `ChatPromptTemplate` to organize the messages.\n",
        "3. Pass the prompt into the model you have loaded before.\n",
        "4. Use `StrOutputParser` to return a plain string.\n",
        "\n",
        "Your final chain should take a dictionary with a **person_name** key and return a brief description about that player."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gMEB4D4z8r0h"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# Create a simple prompt template with a human message and an AI message\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    AIMessagePromptTemplate.from_template(\"You are a helpful assistant who provides a brief explanation about the named player...\"\n",
        "    ),\n",
        "    HumanMessagePromptTemplate.from_template(\"tell me about this playar {person_name}\"),\n",
        "])\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "# Create a simple chain with the prompt, LLM, and output parser. The goal is to generate a response to the prompt and parse the output as a string.\n",
        "simple_chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGGAq5Atcl-3",
        "outputId": "d4c98aef-2f40-4429-844e-0e952fac82a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI: You are a helpful assistant who provides a brief explanation about the named player...\n",
            "Human: tell me about this playar Kylian Mbappé's career.\n",
            "\n",
            "AI:Kylian Mbappé is one of France and Paris Saint-Germain (PSG)'s most celebrated footballers. Born on December 20, 1998 in Marseille, he has made an immediate impact since his professional debut at just sixteen years old with Monaco FC before joining PSG where he's currently playing as their forward.\n",
            "Mbappé debuted for the French national team when only seventeen-years-old during Euro 2016 qualifiers against San Marino under Didier Deschamps' management. He quickly established himself by being called up to captain the Under-21 side ahead-of-schedule due to injury issues amongst other players which saw him lead them from defeating Bosnia-Herzegovina and Bulgaria all way through to winning European Championship Gold medals prior to competing alongside senior teammates Thierry Henry & Karim Benzema within UEFA EURO 2016 final hosted Belgium was ultimately defeated by Spain via penalties following extra-time stoppage time after regulation ended level drawn - ending France’s hopes of becoming World Champions.\n",
            "In terms of club achievements; throughout his illustrious journey thus far across both sides including various trophies won along route such Premier League Cup Final appearances multiple league titles domestically whilst internationally claiming victory over nations like Germany Italy Portugal Greece Denmark etcetera meanwhile earning countless awards individual accolades too namely Ballon d’Or twice consecutively consecutive winners between 2020 – present period\n",
            "The last recorded updates show that as per current standings or available data till November 1st 2023 might not be accurate please do refer trusted sources whenever needed regarding recent statistics while discussing specific events occurring post said date range accordingly....\n",
            "\n"
          ]
        }
      ],
      "source": [
        "answer = simple_chain.invoke({\"person_name\": \"Kylian Mbappé\"})\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hq8wDeKK8r0h"
      },
      "source": [
        "#### Question 3: (2 points)\n",
        "\n",
        "1. Write about the objectives behind the creation of `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes. What they actually do and how should we use them? Write a brief description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kx80OHi9Kcl"
      },
      "source": [
        "The `HumanMessagePromptTemplate` and `AIMessagePromptTemplate` classes are designed to help developers build structured and role-specific prompts for chat-based language models. These templates provide an explicit way to define the conversational context between the user and the AI assistant, aligning with how modern chat models (such as those from OpenAI, Anthropic, or Meta) are trained—using sequences of messages with designated roles.\n",
        "\n",
        "### **What They Do**\n",
        "\n",
        "* **`HumanMessagePromptTemplate`** represents a prompt originating from the user. It is used to define input content that mimics what a human might say in a conversation. This message is treated as a question, instruction, or request from the user.\n",
        "\n",
        "* **`AIMessagePromptTemplate`** represents the AI assistant's response. It is used to simulate a reply from the model, either to demonstrate a desired response or to structure a multi-turn dialogue.\n",
        "\n",
        "These templates are components used within a `ChatPromptTemplate`, which combines a sequence of message templates into a complete conversational prompt that is sent to the model.\n",
        "\n",
        "### **Why They Are Important**\n",
        "\n",
        "Language models that support chat functionality are trained on data where each message is explicitly labeled with a role (e.g., `user`, `assistant`, or `system`). To effectively interact with such models, prompts must be formatted in the same way. These templates enforce this structure, ensuring the model receives input in the format it expects.\n",
        "\n",
        "This alignment with the model's training setup is essential for:\n",
        "\n",
        "* Maintaining clarity about which party (human or AI) is speaking.\n",
        "* Supporting multi-turn dialogue with clear role delineation.\n",
        "* Enabling predictable and instruction-following behavior from the model.\n",
        "\n",
        "\n",
        "### **How to Use Them**\n",
        "\n",
        "These templates are typically used within a `ChatPromptTemplate`. For example:\n",
        "\n",
        "```python\n",
        "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    HumanMessagePromptTemplate.from_template(\"Who is {person_name}?\"),\n",
        "    AIMessagePromptTemplate.from_template(\"Here is some information about {person_name}.\")\n",
        "])\n",
        "```\n",
        "\n",
        "This structure enables the developer to substitute `{person_name}` with user-provided input and simulate both user input and AI response within a prompt chain.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoN6LFaW8r0h"
      },
      "source": [
        "### 2.3 JSON Chain (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F56RIBhk8r0i"
      },
      "source": [
        "#### Completion 3: (1 point)\n",
        "\n",
        "Now we want to improve the chain to extract data from the model response. Modify the existing prompt to request information about a football player, such as:\n",
        "- full name\n",
        "- nationality\n",
        "- age\n",
        "- current club\n",
        "- position\n",
        "\n",
        "In this chain, you can use `SystemMessagePromptTemplate` as well.\n",
        "At the end, use `JsonOutputParser` to parse the model's output and return a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ztIkrpmk8r0i"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "\n",
        "# Create the prompt template.\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\n",
        "    \"Answer the user query as a JSON object\"\n",
        "    \"Your response should be formatted as a valid JSON object with the following fields:\"\n",
        "    \"full_name, nationality, age, current_club, position.\"\n",
        "    ),\n",
        "    HumanMessagePromptTemplate.from_template(\n",
        "        \"Tell me about the player {player_name} and provide their full name, nationality, age, current club, and position.\"\n",
        "        ),\n",
        "    AIMessagePromptTemplate.from_template(\n",
        "        \"I'll provide information about {player_name} in JSON format.\"\n",
        "        )\n",
        "])\n",
        "\n",
        "# Use JsonOutputParser to parse the response as a dictionary\n",
        "output_parser = JsonOutputParser()\n",
        "\n",
        "# Define the chain. This time, we want output to be parsed as JSON, using the JsonOutputParser.\n",
        "json_chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVy5yaAJi3NU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49ca1ee-0cb5-465d-a8d8-49da4752f22a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'full_name': 'Lionel Andrés Messía',\n",
              " 'nationality': 'Argentinean',\n",
              " 'age': 'As of October 2019, he was turning 34 years old.',\n",
              " 'current_club': 'Paris Saint-Germain (PSG)',\n",
              " 'position': 'Attacking Midfielder'}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "json_chain.invoke({\"player_name\": \"Lionel Messi\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xMWli_Ocl-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7389624-4a72-428d-d3b7-f9a74dc378ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lionel Messi:\n",
            "  full_name: Lionel Andrés Messía\n",
            "  nationality: Argentinean\n",
            "  age: 35\n",
            "  current_club: {'club_name': 'Paris Saint-Germain (PSG)', 'league': 'Ligue 1'}\n",
            "  position: ['Forward', 'Attacking Midfielder']\n",
            "\n",
            "Cristiano Ronaldo:\n",
            "  full_name: Cristiano Ronaldo dos Santos Aveiro\n",
            "  nationality: Portuguese (by birth), Naturalized Brazilian citizen since November 2007,\n",
            "                    Previously had Portuguese citizenship),\n",
            "  \n",
            "\n",
            "Kylian Mbappé:\n",
            "  full_name: Kylian Zerro\n",
            "  nationality: French\n",
            "  age: Not provided by AI\n",
            "  current_club: Paris Saint-Germain (PSG)\n",
            "  position: Forward\n",
            "\n",
            "Neymar:\n",
            "  full_name: Neymar Gabriel Marcel Bolsonaro Júnior\n",
            "  nationality: Brazilian (born in Brazil)\n",
            "  age: As of October 3, 1989, he would have turned off 34 years old.\n",
            "  current_club: Paris Saint-Germain FC\n",
            "  position: Forward\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Batch the requests for multiple\n",
        "batch_questions = [\n",
        "  {\"player_name\": \"Lionel Messi\"},\n",
        "  {\"player_name\": \"Cristiano Ronaldo\"},\n",
        "  {\"player_name\": \"Kylian Mbappé\"},\n",
        "  {\"player_name\": \"Neymar\"}\n",
        "]\n",
        "answers = json_chain.batch(batch_questions)\n",
        "\n",
        "# Print the extracted information\n",
        "for (q, a) in zip(batch_questions, answers):\n",
        "  print(f\"{q['player_name']}:\")\n",
        "  for key, value in a.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ0ogQiMcl-4"
      },
      "source": [
        "#### Report 1: (1 point)\n",
        "\n",
        "Explain the challenges you faced in this step. How did you manage to solve them? How could the parameters you used in the text generation pipeline affect the model’s output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU6K2Pfl9Kcm"
      },
      "source": [
        "# Report 1: Challenges faced and solutions\n",
        "When implementing the JSON chain, several challenges may arise:\n",
        "\n",
        "*  **JSON Formatting Issues:** The primary challenge is ensuring that the LLM consistently produces valid JSON. Language models sometimes generate malformed JSON with missing quotes, commas, or brackets. This is why we switched from Phi-4-mini-instruct to Qwen2.5-7B-instruct, which has better JSON capabilities.\n",
        "\n",
        "*  **Hallucinations in Player Data:** The model might generate incorrect information about players, especially for less famous ones or regarding recent transfers/age changes.\n",
        "\n",
        "*  **Parsing Errors:** Even with explicit instructions, the model might include explanatory text before or after the JSON, causing parsing failures.\n",
        "\n",
        "**Solutions implemented:**\n",
        "\n",
        "\n",
        "1.  **Using a clear SystemMessagePromptTemplate** with explicit formatting instructions\n",
        "Including the AI message template to reinforce the JSON requirement\n",
        "\n",
        "2. **Using the JsonOutputParser** to handle parsing and provide clear error messages\n",
        "Setting appropriate generation parameters: lower temperature (around 0.7) for more consistent outputs, and adequate repetition_penalty to avoid syntax errors from repeated tokens\n",
        "The text generation parameters significantly affect the model’s output:\n",
        "\n",
        "3. **Temperature:** Lower values (0.3-0.5) produce more consistent JSON structure but might sacrifice accuracy of information\n",
        "\n",
        "4. **max_new_tokens:** Must be sufficient to allow complete JSON responses\n",
        "repetition_penalty: Helps prevent repetitive syntax errors in JSON\n",
        "top_p/top_k: More restrictive values help maintain proper JSON syntax by limiting unexpected tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHyPzbMD9Kcm"
      },
      "source": [
        "#### Question 4: (2 points)\n",
        "\n",
        "1. How sampling parameters such as *temperature*, *top_p*, and *top_k* can affect our JSON pipeline? Answer the question with respect to the format and content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAvDZVtk9Kcn"
      },
      "source": [
        "# How Sampling Parameters (*temperature*, *top_p*, *top_k*) Affect a JSON Pipeline\n",
        "\n",
        "When using language models to generate structured JSON outputs—especially in pipelines that rely on parsers like `JsonOutputParser` or `PydanticOutputParser`—the sampling parameters you choose can have a significant impact on both the **format** and **content** of the generated JSON.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Overview of Sampling Parameters\n",
        "\n",
        "- **Temperature:** Controls randomness in token selection. Lower values make output more deterministic; higher values increase creativity and variability.\n",
        "- **top_p (nucleus sampling):** Limits token selection to the smallest set whose cumulative probability exceeds *p*. Lower values make output more focused; higher values allow more diverse possibilities.\n",
        "- **top_k:** Limits token selection to the top *k* most probable tokens at each step. Lower values restrict choices; higher values offer more variety.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Impact on JSON Format\n",
        "\n",
        "### **Potential Issues**\n",
        "\n",
        "- **Malformed JSON:** Higher temperature, top_p, or top_k increase the chance of the model generating output that is not valid JSON (e.g., missing brackets, extra commas, or incorrect structure).\n",
        "- **Inconsistent Field Names:** Sampling diversity may cause the model to use synonyms or alternative keys, breaking downstream parsing.\n",
        "- **Unexpected Data Types:** The model might generate values in unexpected formats (e.g., string instead of integer), leading to parsing or validation errors.\n",
        "\n",
        "### **Best Practices**\n",
        "\n",
        "- Use **lower temperature** (e.g., 0.0–0.3) and conservative top_p/top_k to encourage strict adherence to the expected JSON schema.\n",
        "- Always validate the model output before passing it to downstream parsers.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Impact on JSON Content\n",
        "\n",
        "### **Potential Issues**\n",
        "\n",
        "- **Content Variability:** Higher sampling settings make the model more creative, which can be beneficial for open-ended fields but risky for fields expecting a fixed set of values.\n",
        "- **Hallucinated or Irrelevant Fields:** The model may invent new fields or add irrelevant information, which can break Pydantic or JSON validation.\n",
        "- **Reduced Reproducibility:** Higher diversity means repeated runs with the same prompt may yield different outputs, which can be problematic for deterministic workflows.\n",
        "\n",
        "### **Best Practices**\n",
        "\n",
        "- For pipelines requiring **consistent and reliable content** (e.g., data extraction, structured information), use lower sampling settings.\n",
        "- For tasks where **creative or varied content** is desired (e.g., story generation), higher settings may be acceptable, but always post-process and validate.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Summary Table\n",
        "\n",
        "| Parameter    | Effect on Format                  | Effect on Content                | Recommendation for JSON Pipelines         |\n",
        "|--------------|-----------------------------------|----------------------------------|-------------------------------------------|\n",
        "| temperature  | Higher = more format errors       | Higher = more creative content   | Keep low (≤ 0.3) for structured outputs   |\n",
        "| top_p        | Higher = less predictable format  | Higher = more diverse responses  | Keep low (≤ 0.9) for strict schemas       |\n",
        "| top_k        | Higher = more format variability  | Higher = more content variety    | Keep low (≤ 40) for reliable parsing      |\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Example\n",
        "\n",
        "Suppose your prompt expects:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RF_aXt6dC9g"
      },
      "source": [
        "### 2.4 (Optional) Tool use: Web search (2 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjkcoqElcl-4"
      },
      "source": [
        "#### Completion 4: (2 points)\n",
        "\n",
        "Add context about each player by using web search. For this purpose, you need to use a web search tool, and write a function to check the output of the search tool."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install duckduckgo-search"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8ZZ8WJVP3Uk",
        "outputId": "7064fac2-ccb4-487a-e7c2-d499647c3a91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/3.3 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
        "from operator import itemgetter\n",
        "import time\n",
        "import random\n",
        "\n",
        "# 1. Initialize the DuckDuckGo Search tool\n",
        "search = DuckDuckGoSearchRun()\n",
        "\n",
        "# 2. Define a function to use the search tool and process its output\n",
        "# This function will take the entire input dictionary (e.g., {\"player_name\": \"Lionel Messi\"})\n",
        "# and return the processed search context.\n",
        "def get_player_context(data: dict) -> str:\n",
        "    player_name = data[\"player_name\"]\n",
        "    # Construct a more specific search query for player details\n",
        "    search_query = f\"football player {player_name} current club position nationality age\"\n",
        "    search_result = search.run(search_query)\n",
        "\n",
        "    time.sleep(random.uniform(2,5))\n",
        "\n",
        "    if search_result:\n",
        "        # Simple truncation of the search result to manage context window size.\n",
        "        # Keeping around 1500 characters, which is usually enough for a brief context.\n",
        "        return search_result[:1500] + \"...\" if len(search_result) > 1500 else search_result\n",
        "    return \"No specific web information found for this player from search.\"\n",
        "\n",
        "# 3. Define the RAG-enabled prompt template.\n",
        "# This new prompt will include a placeholder for `search_context`.\n",
        "rag_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(\n",
        "    \"You are a helpful assistant that extracts football player details from the provided context. \"\n",
        "    \"Respond ONLY with a valid JSON object containing the following fields: \"\n",
        "    \"full_name, nationality, age, current_club, position. \"\n",
        "    \"If any information is not explicitly available in the provided context, use 'N/A' or null for that field. \"\n",
        "    \"DO NOT include any conversational text or explanations outside the JSON object. \"\n",
        "    \"Prioritize information from the search context for accuracy.\"\n",
        "    ),\n",
        "    HumanMessagePromptTemplate.from_template(\n",
        "        \"Here is some web search context about {player_name}:\\n{search_context}\\n\\n\"\n",
        "        \"Extract the full name, nationality, age, current club, and position for {player_name} based on the context.\"\n",
        "    ),\n",
        "    # Removed the AIMessagePromptTemplate as it adds conversational text\n",
        "])\n",
        "\n",
        "# Use JsonOutputParser to parse the response as a dictionary\n",
        "output_parser = JsonOutputParser()\n",
        "\n",
        "# Define the chain.\n",
        "# We use RunnableParallel to prepare the inputs for the rag_prompt.\n",
        "# - \"player_name\": passes the original player_name from the batch_questions input directly.\n",
        "# - \"search_context\": calls get_player_context function with the input dict to fetch search results.\n",
        "footballer_chain = (\n",
        "    {\n",
        "        # Extract the 'player_name' string from the input dictionary and pass it.\n",
        "        \"player_name\": itemgetter(\"player_name\"),\n",
        "        # The RunnableLambda receives the full input dictionary, which get_player_context expects.\n",
        "        \"search_context\": RunnableLambda(get_player_context),\n",
        "    }\n",
        "    | rag_prompt # The prompt expects 'player_name' (string) and 'search_context' (string)\n",
        "    | llm # The LLM generates the response based on the prompt\n",
        "    | output_parser # The JsonOutputParser parses the LLM's output into a Python dictionary\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "X1nutIihR3k9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "7UhjgFJTcl-4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7869a3ea-816f-46a0-abf7-6de2ab85a680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7acebfefd240>\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
            "/usr/local/lib/python3.11/dist-packages/langchain_community/utilities/duckduckgo_search.py:63: RuntimeWarning: This package (`duckduckgo_search`) has been renamed to `ddgs`! Use `pip install ddgs` instead.\n",
            "  with DDGS() as ddgs:\n",
            "sys:1: ResourceWarning: Unclosed socket <zmq.Socket(zmq.PUSH) at 0x7acee0173f50>\n",
            "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lionel Messi:\n",
            "  full_name: Lionel Andrés Messi\n",
            "  nationality: Argentine\n",
            "  age: 37\n",
            "  current_club: Inter Miami\n",
            "  position: Forward\n",
            "\n",
            "Cristiano Ronaldo:\n",
            "  full_name: Cristiano Ronaldo dos Santos Aveiro\n",
            "  nationality: Portuguese\n",
            "  age: None\n",
            "  current_club: Al-Nassr FC\n",
            "  position: Centre Forward\n",
            "\n",
            "Kylian Mbappé:\n",
            "  full_name: Kylian Mbappé Lottin\n",
            "  nationality: French\n",
            "  age: None\n",
            "  current_club: Real Madrid\n",
            "  position: Forward\n",
            "\n",
            "Neymar:\n",
            "  full_name: Neymar Jr.\n",
            "  nationality: Brazilian\n",
            "  age: None\n",
            "  current_club: None\n",
            "  position: None\n",
            "\n",
            "Declan Rice:\n",
            "  full_name: Declan Rice\n",
            "  nationality: Irish/North American Dual Citizenship/Belonging - Born in England but grew up largely in North America after his parents moved there.\n",
            "  age: None\n",
            "  current_club: None\n",
            "  position: Midfielder\n",
            "\n",
            "Trent Alexander-Arnold:\n",
            "  full_name: Trent Alexander-Arnold\n",
            "  nationality: None\n",
            "  age: None\n",
            "  current_club: None\n",
            "  position: None\n",
            "\n",
            "John Stones:\n",
            "  full_name: John Stones\n",
            "  nationality: British\n",
            "  age: None\n",
            "  current_club: None\n",
            "  position: None\n",
            "\n",
            "Alphonso Davies:\n",
            "  full_name: Alphonso Davies\n",
            "  nationality: Canadian-Guinean\n",
            "  age: None\n",
            "  current_club: Bayern Munich\n",
            "  position: ['left-back', 'winger']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Batch the requests for multiple\n",
        "batch_questions = [\n",
        "  {\"player_name\": \"Lionel Messi\"},\n",
        "  {\"player_name\": \"Cristiano Ronaldo\"},\n",
        "  {\"player_name\": \"Kylian Mbappé\"},\n",
        "  {\"player_name\": \"Neymar\"},\n",
        "  {\"player_name\": \"Declan Rice\"},\n",
        "  {\"player_name\": \"Trent Alexander-Arnold\"},\n",
        "  {\"player_name\": \"John Stones\"},\n",
        "  {\"player_name\": \"Alphonso Davies\"}\n",
        "]\n",
        "answers = footballer_chain.batch(batch_questions)\n",
        "\n",
        "# Print the extracted information\n",
        "for (q, a) in zip(batch_questions, answers):\n",
        "  print(f\"{q['player_name']}:\")\n",
        "  for key, value in a.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS8as30J8r0i"
      },
      "source": [
        "## 3. Build a RAG pipeline (26 points + 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aM-prY48r0i"
      },
      "source": [
        "In this section, We use a subset of [RecipeNLG](https://recipenlg.cs.put.poznan.pl) dataset to build our RAG pipelines. The dataset contains recipes and their corresponding instructions.\n",
        "\n",
        "You can download the subset from [this google drive link](https://drive.google.com/file/d/1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5/view?usp=sharing) or from the course website."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW2JuoTzNNek"
      },
      "source": [
        "### 3.1 Load and prepare the dataset (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGWVvgLqNNek"
      },
      "source": [
        "#### Completion 5: (4 point)\n",
        "\n",
        "First, you should load the dataset, which is stored in a CSV file. and converting it to a `datasets.Dataset` object.\n",
        "\n",
        "The dataset contains the following columns:\n",
        "- **title**: The name of the recipe\n",
        "- **ingredients**: A list of ingredients used in the recipe, including quantities and preparation methods\n",
        "- **directions**: The instructions for preparing the recipe, presented as a list of sequential steps\n",
        "- **NER**: A list of named entities representing the core food items and cooking components extracted from each recipe, without quantities or preparation instructions.\n",
        "\n",
        "**Attention**: You should carefully process list objects (ingredients, directions, and NER) and convert them to a string document.\n",
        "\n",
        "**Attention 2**: The provided dataset, has 5k recipes. You can use a smaller subset of the dataset for your experiments. For example, you can use the first 100 recipes for your experiments or more, based on your resource limitation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YIkZv62ZNNek"
      },
      "outputs": [],
      "source": [
        "!pip install -q gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6cVk_djLNNek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba058f26-4871-4e62-9aed-88605395cff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5\n",
            "To: /content/data_5000.csv\n",
            "100% 5.92M/5.92M [00:00<00:00, 37.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown 1mgPcQKc7-SaWVyxaJ404L6dGkQvODca5 -O data_5000.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import ast # For safely evaluating string representations of Python literals\n",
        "\n",
        "# Load the CSV file into a pandas DataFrame\n",
        "df = pd.read_csv(\"data_5000.csv\")\n",
        "\n",
        "# Process the list columns: ingredients, directions, NER\n",
        "# Convert string representation of lists to actual Python lists, then join them into a single string.\n",
        "for col in ['ingredients', 'directions', 'NER']:\n",
        "    df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) # Convert string to list\n",
        "    df[col] = df[col].apply(lambda x: \" \".join(x) if isinstance(x, list) else x) # Join list elements into a single string\n",
        "\n",
        "# Optional: Use a smaller subset (e.g., first 100 recipes)\n",
        "df = df.head(100) # Use the first 100 recipes for faster experimentation\n",
        "\n",
        "# Convert the pandas DataFrame to a datasets.Dataset object\n",
        "dataset = Dataset.from_pandas(df)\n",
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfEeqpaoo0wp",
        "outputId": "113af2b4-68df-4cce-ea1c-17d000ff71a1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['Unnamed: 0', 'title', 'ingredients', 'directions', 'NER'],\n",
            "    num_rows: 100\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fbTwX4LvC3H",
        "outputId": "6ba7332e-0453-48ad-995d-d7e8c3c9ad93"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Unnamed: 0': 0,\n",
              " 'title': 'Worlds Best Mac and Cheese',\n",
              " 'ingredients': '6 ounces penne 2 cups Beechers Flagship Cheese Sauce (recipe follows) 1 ounce Cheddar, grated (1/4 cup) 1 ounce Gruyere cheese, grated (1/4 cup) 1/4 to 1/2 teaspoon chipotle chili powder (see Note) 1/4 cup (1/2 stick) unsalted butter 1/3 cup all-purpose flour 3 cups milk 14 ounces semihard cheese (page 23), grated (about 3 1/2 cups) 2 ounces semisoft cheese (page 23), grated (1/2 cup) 1/2 teaspoon kosher salt 1/4 to 1/2 teaspoon chipotle chili powder 1/8 teaspoon garlic powder (makes about 4 cups)',\n",
              " 'directions': 'Preheat the oven to 350 F. Butter or oil an 8-inch baking dish. Cook the penne 2 minutes less than package directions. (It will finish cooking in the oven.) Rinse the pasta in cold water and set aside. Combine the cooked pasta and the sauce in a medium bowl and mix carefully but thoroughly. Scrape the pasta into the prepared baking dish. Sprinkle the top with the cheeses and then the chili powder. Bake, uncovered, for 20 minutes. Let the mac and cheese sit for 5 minutes before serving. Melt the butter in a heavy-bottomed saucepan over medium heat and whisk in the flour. Continue whisking and cooking for 2 minutes. Slowly add the milk, whisking constantly. Cook until the sauce thickens, about 10 minutes, stirring frequently. Remove from the heat. Add the cheeses, salt, chili powder, and garlic powder. Stir until the cheese is melted and all ingredients are incorporated, about 3 minutes. Use immediately, or refrigerate for up to 3 days. This sauce reheats nicely on the stove in a saucepan over low heat. Stir frequently so the sauce doesnt scorch. This recipe can be assembled before baking and frozen for up to 3 monthsjust be sure to use a freezer-to-oven pan and increase the baking time to 50 minutes. One-half teaspoon of chipotle chili powder makes a spicy mac, so make sure your family and friends can handle it! The proportion of pasta to cheese sauce is crucial to the success of the dish. It will look like a lot of sauce for the pasta, but some of the liquid will be absorbed.',\n",
              " 'NER': 'penne Beechers Flagship Cheese Sauce Cheddar Gruyere cheese chipotle chili powder butter flour milk semihard cheese semisoft cheese kosher salt chipotle chili powder garlic'}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WF2MjTYelRfB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4741663-4ad6-4eef-f746-c178f553e6cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 100\n"
          ]
        }
      ],
      "source": [
        "# In this cell, you should store the dataset, as a list of `langchain_core.documents.Document` objects, which can simplify your future steps.\n",
        "# You should decide how to convert the dataset to documents\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "documents: list[Document] = [Document(page_content=f\"Ingredients: {row['ingredients']}\\n\\nDirections: {row['directions']}\",\n",
        "         metadata={\"title\": row['title']})\n",
        "         for row in dataset\n",
        "                             ]\n",
        "\n",
        "print(f\"Number of documents: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2bHNwp3sNNel",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5049e5ff-d8bd-4c2d-d23a-55a942aa05d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of chunks: 157\n"
          ]
        }
      ],
      "source": [
        "# Now, you should use a splitter to divide long texts into smaller, manageable chunks so they can fit within the context window of language models or retrievers.\n",
        "# Use `RecursiveCharacterTextSplitter` to split the documents into smaller chunks, ans set the `chunk_size` and `chunk_overlap` parameters accordingly.\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Initialize the text splitter with appropriate chunk_size and chunk_overlap\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # You can adjust this based on your LLM's context window and desired chunk size\n",
        "    chunk_overlap=200, # This ensures some context is carried over between chunks\n",
        "    length_function=len, # Use len to measure chunk size by characters\n",
        "    is_separator_regex=False, # Set to True if separators are regex patterns\n",
        ")\n",
        "\n",
        "# Split the documents into smaller chunks\n",
        "chunks = text_splitter.split_documents(documents) # 'documents' is the list you created in the previous step\n",
        "\n",
        "print(f\"Number of chunks: {len(chunks)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN7dSqY-NNel"
      },
      "source": [
        "### 3.2 Sparse Retriever (3 points)\n",
        "\n",
        "In this section, we would create a sparse retriever for our RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elzh6QSSNNem"
      },
      "source": [
        "#### Question 5: (2 points)\n",
        "\n",
        "1. Explain how a sparse retriever like TF-IDF represents documents and queries. How does this representation influence which documents are retrieved?\n",
        "2. Sparse retrievers rely on exact token matches between queries and documents. What are the strengths and weaknesses of this approach, especially compared to dense retrievers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VXT5RG79Kcw"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvmpQ-BtNNem"
      },
      "source": [
        "#### Completion 6: (1 point)\n",
        "\n",
        "Complete the code cells below to create a sparse retriever, which would be later used in our RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rq6DS9wNNem"
      },
      "outputs": [],
      "source": [
        "# Prepare your retriever. For this section, you should use a sparse retriever such as `TFIDF` or `BM25`.\n",
        "# We want our retriever to retrieve the first 3 chunks that are most relevant to the query.\n",
        "\n",
        "\n",
        "sparse_retriever ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7IoZQioNNem"
      },
      "outputs": [],
      "source": [
        "# Query below is related to `Zucchini Nut Bread` recipe.\n",
        "\n",
        "Sample_query = \"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\"\n",
        "\n",
        "# Use the sparse retriever to get the most relevant chunks for the query\n",
        "retrieved_chunks =\n",
        "\n",
        "# Now, see what chunks were retrieved\n",
        "for i, chunk in enumerate(retrieved_chunks, start=1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    # print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qnbeJnz8r0k"
      },
      "source": [
        "### 3.3 Semantic Retriever (4 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgEphnmn8r0k"
      },
      "source": [
        "#### Question 6: (2 point)\n",
        "\n",
        "1. How does a semantic retriever represent documents and queries differently from a sparse retriever? Explain why this helps in cases where there are no exact word overlaps.\n",
        "2. What role do embedding models (e.g., sentence-transformers) play in semantic retrieval? How does the choice of embedding model affect the retriever’s performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loa5uJFE9Kcx"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1GCzoOXUf8c"
      },
      "source": [
        "#### Completion 7: (2 point)\n",
        "\n",
        "Let's create a semantic retriever. We would use `BAAI/bge-small-en` as our embedding model, and `FAISS` as our vector store. Complete the code cells below to create a semantic retriever, which would be later used in our RAG pipeline.\n",
        "\n",
        "As explained before, we want our retriever to retrieve the first 3 most relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6NzUEAs8r0k"
      },
      "outputs": [],
      "source": [
        "# Code here\n",
        "\n",
        "embedding_model =\n",
        "semantic_retriever ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5Uc59HkUf8c"
      },
      "outputs": [],
      "source": [
        "# Query below is related to `Zucchini Nut Bread` recipe.\n",
        "\n",
        "Sample_query = \"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\"\n",
        "\n",
        "# Use the semantic retriever to get the most relevant chunks for the query\n",
        "retrieved_chunks =\n",
        "\n",
        "# Now, see what chunks were retrieved\n",
        "for i, chunk in enumerate(retrieved_chunks, start=1):\n",
        "    print(f\"Chunk {i}:\")\n",
        "    # print(chunk.page_content)\n",
        "    print(\"Metadata:\", chunk.metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYbF75HP2bBc"
      },
      "source": [
        "### 3.4 Create RAG pipelines (6 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_KnISF32bBc"
      },
      "source": [
        "#### Question 7: (2 points)\n",
        "\n",
        "1. What are the main components of a RAG system, and how do they interact during inference? Describe the flow from user input to model output.\n",
        "2. Describe two different strategies for integrating retrieved context into the prompt. What are the trade-offs between these approaches?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TJ2_4Bn9Kcz"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avtuGMuT2bBc"
      },
      "source": [
        "#### Completion 8: (4 points)\n",
        "\n",
        "Follow the instructions below to build a RAG pipeline using the retrievers you created in the previous sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dqo5m1La2bBc"
      },
      "outputs": [],
      "source": [
        "Sample_query = \"\"\"\\\n",
        "The kitchen smells warm and sweet already. \\\n",
        "I’ve beaten the eggs until they’re nice and frothy, then slowly mixed in the sugar, vegetable oil, and vanilla. \\\n",
        "it’s turned into a thick, glossy batter, smooth and golden. \\\n",
        "I’ve just stirred in the fresh, grated zucchini, and it’s added a slightly textured, green-flecked look to the mix. \\\n",
        "It’s moist, with a nice balance of richness and freshness from the zucchini.\n",
        "\n",
        "What is your best guess about what am I cooking?\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxybNSeT2bBc"
      },
      "outputs": [],
      "source": [
        "# We are going to use \"microsoft/Phi-4-mini-instruct\" as our LLM again. If you need, load it again here and as before.\n",
        "\n",
        "model_id = \"microsoft/Phi-4-mini-instruct\"\n",
        "tokenizer =\n",
        "model =\n",
        "pipe =\n",
        "llm ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-_gpU7u2bBc"
      },
      "outputs": [],
      "source": [
        "# First, we need to define a new chat template, that provide the retrieved documents as context to the LLM.\n",
        "from langchain_core.prompts import SystemMessagePromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2YU0sdZa2bBc"
      },
      "outputs": [],
      "source": [
        "# Now, let's create a simple RAG pipeline, using the sparse retriever. Note that we need the retrieved context as part of the output, so that we can later use it for evaluation.\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "sparse_rag = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : (input)  populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : (output) chunks retrieved by the sparse retriever, based on the \"question\" value\n",
        "    # \"response\" : (output) the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #                       into the LLM and stored in a key called \"response\"\n",
        "\n",
        ")\n",
        "\n",
        "# Now, let's test the sparse RAG pipeline with a sample query.\n",
        "response =\n",
        "print(response[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRy88ZMi2bBc"
      },
      "outputs": [],
      "source": [
        "# For this cell, everything is the same as the previous cell, except that we are using the semantic retriever instead of the sparse retriever.\n",
        "\n",
        "semantic_rag = (\n",
        "    # The same as previous cell, but using the semantic retriever instead of the sparse retriever\n",
        ")\n",
        "\n",
        "# Now, let's test the semantic RAG pipeline with a sample query.\n",
        "response =\n",
        "print(response[\"response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvYodfcg2bBd"
      },
      "outputs": [],
      "source": [
        "# Finally, let's try the same query with the LLM directly, without any retrieval.\n",
        "response =\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mCOh9_i2bBd"
      },
      "source": [
        "### 3.5 Evaluate our pipelines (9 points)\n",
        "\n",
        "In this section, we are going to evaluate our RAG pipelines. First, we would design 5 queries to evaluate our RAG pipelines and our LLM alone."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDgQ1uNE2bBd"
      },
      "source": [
        "#### Completion 9: (1 point)\n",
        "\n",
        "Add 4 more queries, similar to the example. The examples would be based on the first 100 recipes of our dataset.\n",
        "We would keep the title of the recipe that we used to create the query, for future reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2pV6lw_2bBd"
      },
      "outputs": [],
      "source": [
        "queries = [\n",
        "    {\n",
        "        \"title\": \"Balsamic Chicken Pasta with Fresh Cheese\",\n",
        "        \"query\": \"I am cooking dinner. Here is what my kitchen looks like:\\nThe linguine is cooked and set aside. The red bell peppers are soft and slightly caramelized. The balsamic dressing is mixed with garlic, salt, pepper, and fresh basil. Each component is ready in its bowl, colorful and aromatic.\\n\\nWhat should I do as my next step?\"\n",
        "    },\n",
        "    # Add 4 more\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Gglt-XH2bBd"
      },
      "outputs": [],
      "source": [
        "from textwrap import fill\n",
        "questions = [{\"question\": q[\"query\"]} for q in queries]\n",
        "\n",
        "llm_responses = llm.batch([q[\"query\"] for q in queries])\n",
        "sparse_rag_responses = sparse_rag.batch(questions)\n",
        "semantic_rag_responses = semantic_rag.batch(questions)\n",
        "\n",
        "for query, r1, r2, r3 in zip(queries, llm_responses, sparse_rag_responses, semantic_rag_responses):\n",
        "    print(f'{query[\"title\"]}:')\n",
        "    print(f'  - Without  RAG: {fill(r1, width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print(f'  - Sparse   RAG: {fill(r2[\"response\"], width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print(f'  - Semantic RAG: {fill(r3[\"response\"], width=90, initial_indent=\"\", subsequent_indent=\" \"*18)}')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOJLRpnD2bBd"
      },
      "source": [
        "#### Report 2: (2 points)\n",
        "\n",
        "Write a report about the experiments above. Your report should address the following:\n",
        "1. Compare the quality of the answers. In which cases did Sparse or Semantic RAG help improve the response? Was there any example where it hurt the performance?\n",
        "2. Discuss the differences between Sparse and Semantic RAG. Based on your examples, which one seems more effective and why?\n",
        "3. Any surprising findings or patterns? Did anything behave differently than you expected?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGpTaetj9Kc2"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJIR0sJ2bBd"
      },
      "source": [
        "#### Completion 10: (3 points)\n",
        "\n",
        "Now we want to automate the evaluation process. For this purpose, we are going to use the `ragas`. Follow the instructions of each cell to create the evaluation pipeline. To learn more about this framework, please refer to its [get started](https://docs.ragas.io/en/stable/getstarted/) or [how-to](https://docs.ragas.io/en/stable/howtos/) pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxjSZO7V2bBd"
      },
      "outputs": [],
      "source": [
        "!pip install -q ragas rapidfuzz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JG_Q2JCf2bBd"
      },
      "outputs": [],
      "source": [
        "# Load the LLM as ragas llm. For this, we can use the provided wrapper for our existing LLM.\n",
        "\n",
        "ragas_llm ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuL-IP9L2bBd"
      },
      "outputs": [],
      "source": [
        "# Generate 10 test cases, using ragas, based on your documents. You can use a subset of your documents for faster runtime.\n",
        "test_set =\n",
        "\n",
        "\n",
        "test_set.test_data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2O-XwiVs2bBd"
      },
      "outputs": [],
      "source": [
        "test_df = test_set.to_pandas()\n",
        "test_df.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU3QIdVW2bBd"
      },
      "outputs": [],
      "source": [
        "test_questions = test_df[\"question\"].values.tolist()\n",
        "test_ground_truths = test_df[\"ground_truth\"].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAyWxf1u2bBd"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"sparse\": {\n",
        "        \"answers\": [],\n",
        "        \"contexts\": []\n",
        "    },\n",
        "    \"dense\": {\n",
        "        \"answers\": [],\n",
        "        \"contexts\": []\n",
        "    },\n",
        "}\n",
        "\n",
        "for question in test_questions:\n",
        "    q = {\"question\": question}\n",
        "    s_response = sparse_rag.invoke(q)\n",
        "    d_response = semantic_rag.invoke(q)\n",
        "\n",
        "    results[\"sparse\"][\"answers\"].append(s_response[\"response\"])\n",
        "    results[\"sparse\"][\"contexts\"].append([context.page_content for context in s_response[\"context\"]])\n",
        "    results[\"dense\"][\"answers\"].append(d_response[\"response\"])\n",
        "    results[\"dense\"][\"contexts\"].append([context.page_content for context in d_response[\"context\"]])\n",
        "\n",
        "from datasets import Dataset\n",
        "\n",
        "sparse_response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : results[\"sparse\"][\"answers\"],\n",
        "    \"contexts\" : results[\"sparse\"][\"contexts\"],\n",
        "    \"ground_truth\" : test_ground_truths\n",
        "})\n",
        "dense_response_dataset = Dataset.from_dict({\n",
        "    \"question\" : test_questions,\n",
        "    \"answer\" : results[\"dense\"][\"answers\"],\n",
        "    \"contexts\" : results[\"dense\"][\"contexts\"],\n",
        "    \"ground_truth\" : test_ground_truths\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHHmIKiV2bBd"
      },
      "outputs": [],
      "source": [
        "# Load ragas evaluation metrics. We would use all possible metrics, including:\n",
        "# - Faithfulness\n",
        "# - Answer relevancy\n",
        "# - Answer correctness\n",
        "# - retrieved context related metrics\n",
        "\n",
        "metrics = [\n",
        "\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Gn_stOM2bBd"
      },
      "outputs": [],
      "source": [
        "# Use ragas evaluator to report the score of each pipeline, using the metrics defined above.\n",
        "\n",
        "sparse_scores =\n",
        "dense_scores =\n",
        "\n",
        "print(f\"Sparse RAG Score: {sparse_scores}\")\n",
        "print(f\"Dense RAG Score: {dense_scores}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1p1Zsbbv2bBe"
      },
      "outputs": [],
      "source": [
        "sparse_scores.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkrM-3XC2bBe"
      },
      "outputs": [],
      "source": [
        "dense_scores.to_pandas().head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGTkCkXR2bBe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_sparse = pd.DataFrame(list(sparse_scores.items()), columns=['Metric', 'Sparse Retriever'])\n",
        "df_dense = pd.DataFrame(list(dense_scores.items()), columns=['Metric', 'Dense Retriever'])\n",
        "\n",
        "df_merged = pd.merge(df_sparse, df_dense, on='Metric')\n",
        "\n",
        "df_merged['Delta'] = df_merged['Dense Retriever'] - df_merged['Sparse Retriever']\n",
        "\n",
        "df_merged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgCuLLiP2bBe"
      },
      "source": [
        "#### Report 3: (1 point)\n",
        "\n",
        "Compare the automated evaluation (using ragas) with your manual evaluation from the previous step. In your report, make sure to address the following:\n",
        "1. How are the two evaluation methods different? Briefly describe what makes the automated evaluation distinct from your manual judgment process (e.g., consistency, objectivity, criteria used).\n",
        "2.\tDo both evaluations show the same results? Were the rankings or judgments about the quality of responses consistent between your analysis and the automated scores?\n",
        "3.\tIf there were differences, why might that be? Reflect on what factors could lead to different results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnvhxPHK9Kc5"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSIS7xwI2bBe"
      },
      "source": [
        "#### Question 8: (2 points)\n",
        "\n",
        "1. Explain the underlying mechanism and techniques used by `ragas` to evaluate the performance of RAG pipelines. How does it ensure that the evaluation is both comprehensive and relevant to the task?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApBfYdw89Kc6"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrgGWbOG2bBe"
      },
      "source": [
        "### 3.6 (Optional) Other strategy: (3 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12LXj99X2bBe"
      },
      "source": [
        "#### Question 9: (3 points)\n",
        "\n",
        "There are other retriever strategies you can use to improve the performance of your RAG pipeline. In this task:\n",
        "1. Explain what the `MultiQueryRetriever` does and how it can help improve retrieval quality in your pipeline.\n",
        "2. Implement the `MultiQueryRetriever` in your RAG pipeline and evaluate its performance using both manual and automated methods.\n",
        "3. You may also make additional improvements to your pipeline. If you do so, briefly explain what changes you made, how they affect the system, and why they might improve performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX09aqSe9Kc6"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUMa9seb9Kc7"
      },
      "source": [
        "## 4. Read more: (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7zaFJgj9Kc7"
      },
      "source": [
        "#### Cache-Augmented Generation (CAG): (4 points)\n",
        "\n",
        "1. What is Cache-Augmented Generation (CAG)? How does it improve efficiency or performance during generation?\n",
        "2. What are the similarities and differences between Cache-Augmented Generation (CAG) and Retrieval-Augmented Generation (RAG)? In what scenarios might you prefer one over the other?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayjrAeA49Kc7"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbV9U9or9Kc7"
      },
      "source": [
        "#### Multi-modal RAG: (6 points)\n",
        "\n",
        "1. How do models like CLIP enable the embedding of both text and images into a shared vector space? What are the advantages and disadvantages of using a unified embedding space for cross-modal retrieval in RAG systems.\n",
        "2. In systems like [Colpali](https://arxiv.org/pdf/2407.01449), how does dividing document images into patches enhance the retrieval process in multimodal RAG? Explore how patch-based processing preserves structural information and its impact on retrieval accuracy.\n",
        "3. What are the implications of converting non-text modalities (e.g., images) into textual representations for retrieval purposes? Discuss the benefits and drawbacks of grounding all modalities into a primary modality, such as text, in the context of RAG."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "junLQ_x69Kc7"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FN7dSqY-NNel",
        "QvmpQ-BtNNem"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "46220383625f491c9a0e68523c613a1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1fee0b757ed4b10ba4d5178824cb997",
              "IPY_MODEL_9ba8a4c4164c4483acad0aa291dcaf08",
              "IPY_MODEL_a8fbc367d7e14f3c9b5b3b65cd30f9fb"
            ],
            "layout": "IPY_MODEL_15c0b1991d6d4b1cbb03cf89a5b0c9ad"
          }
        },
        "e1fee0b757ed4b10ba4d5178824cb997": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1f1e54d3e414b2ba0907e0a1bb77fc3",
            "placeholder": "​",
            "style": "IPY_MODEL_ec5066c7069b43558d3275c333be99bf",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "9ba8a4c4164c4483acad0aa291dcaf08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a4999c6de92454094db1889e880ea1c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea66e2c0f2d24c569a0564dd039fe9ef",
            "value": 2
          }
        },
        "a8fbc367d7e14f3c9b5b3b65cd30f9fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c31195c5b5ed4e9783d33a7ba7f2a59b",
            "placeholder": "​",
            "style": "IPY_MODEL_f539d2fc979747e08538a59215d0b3cc",
            "value": " 2/2 [00:35&lt;00:00, 16.70s/it]"
          }
        },
        "15c0b1991d6d4b1cbb03cf89a5b0c9ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1f1e54d3e414b2ba0907e0a1bb77fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec5066c7069b43558d3275c333be99bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a4999c6de92454094db1889e880ea1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea66e2c0f2d24c569a0564dd039fe9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c31195c5b5ed4e9783d33a7ba7f2a59b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f539d2fc979747e08538a59215d0b3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}