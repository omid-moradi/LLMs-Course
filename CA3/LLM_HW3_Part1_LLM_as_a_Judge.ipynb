{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ze-S8UKMQx-"
      },
      "source": [
        "# **CA 3 - Part1, LLMs Spring 2025**\n",
        "\n",
        "- **Name:**\n",
        "- **Student ID:**\n",
        "\n",
        "---\n",
        "#### Your submission should be named using the following format: `CA3 - Part1_LASTNAME_STUDENTID.ipynb`.\n",
        "\n",
        "---\n",
        "\n",
        "##### *How to do this problem set:*\n",
        "\n",
        "- Some questions require writing Python code and computing results, and the rest of them have written answers. For coding problems, you will have to fill out all code blocks that say `YOUR CODE HERE`.\n",
        "\n",
        "- For text-based answers, you should replace the text that says ```Your Answer Here``` with your actual answer.\n",
        "\n",
        "- There is no penalty for using AI assistance on this homework as long as you fully disclose it in the final cell of this notebook (this includes storing any prompts that you feed to large language models). That said, anyone caught using AI assistance without proper disclosure will receive a zero on the assignment (we have several automatic tools to detect such cases). We're literally allowing you to use it with no limitations, so there is no reason to lie!\n",
        "\n",
        "---\n",
        "\n",
        "##### *Academic honesty*\n",
        "\n",
        "- We will audit the Colab notebooks from a set number of students, chosen at random. The audits will check that the code you wrote actually generates the answers in your notebook. If you turn in correct answers on your notebook without code that actually generates those answers, we will consider this a serious case of cheating.\n",
        "\n",
        "- We will also run automatic checks of Colab notebooks for plagiarism. Copying code from others is also considered a serious case of cheating.\n",
        "\n",
        "---\n",
        "\n",
        "If you have any further questions or concerns, contact the TAs via email:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2X4-pPZJ2DwU"
      },
      "source": [
        "# Import libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nneldkE1ME1q",
        "outputId": "5a5ba45d-3605-4c1f-9041-69b6f5e7297a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip -q install datasets\n",
        "!pip -q install transformers==4.36.0\n",
        "!pip -q install accelerate flash_attn scikit-learn tqdm datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgivWMATc3ZZ",
        "outputId": "3425dd0a-3138-4d66-c7b2-cf3375d02ba3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  _torch_pytree._register_pytree_node(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n",
        "from sklearn.metrics import accuracy_score, mean_absolute_error, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from collections import Counter\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PjORxmLhzWZ"
      },
      "source": [
        "# 🧩Part 1: Judgement Strategies in LLM as a Judge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLJ2f7-91Lrk"
      },
      "source": [
        "## 1.1 Load Dataset\n",
        "\n",
        "In this assignment, you will explore a dataset commonly used for evaluating feedback and alignment in Large Language Models (LLMs). The goal is to help you become familiar with how such datasets are structured and how to extract meaningful information from them.\n",
        "\n",
        " use the 🤗 datasets library to download the following dataset:\n",
        "\n",
        "> `prometheus-eval/Feedback-Bench`\n",
        "\n",
        "> Link: https://huggingface.co/datasets/prometheus-eval/Feedback-Bench\n",
        "\n",
        "> paper: https://arxiv.org/abs/2310.08491\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aBUfQsW5Vhg5",
        "outputId": "5dd260e6-a101-4db6-9ec9-9fc6e604605e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.36.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.53.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.33.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\n",
            "Requirement already satisfied: flash_attn in /usr/local/lib/python3.11/dist-packages (2.8.0.post2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Downloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from flash_attn) (0.8.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.53.0-py3-none-any.whl (10.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m129.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.33.2-py3-none-any.whl (515 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m108.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fsspec, scikit-learn, huggingface_hub, tokenizers, transformers, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.33.0\n",
            "    Uninstalling huggingface-hub-0.33.0:\n",
            "      Successfully uninstalled huggingface-hub-0.33.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.2\n",
            "    Uninstalling tokenizers-0.15.2:\n",
            "      Successfully uninstalled tokenizers-0.15.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.36.0\n",
            "    Uninstalling transformers-4.36.0:\n",
            "      Successfully uninstalled transformers-4.36.0\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.14.4\n",
            "    Uninstalling datasets-2.14.4:\n",
            "      Successfully uninstalled datasets-2.14.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 fsspec-2025.3.0 huggingface_hub-0.33.2 scikit-learn-1.7.0 tokenizers-0.21.2 transformers-4.53.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "6962d3e6f4ca4bbc90bf5b67834aed8a",
              "pip_warning": {
                "packages": [
                  "datasets",
                  "fsspec",
                  "sklearn",
                  "tokenizers",
                  "transformers"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install --upgrade datasets transformers huggingface_hub accelerate flash_attn scikit-learn tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "cJUUmYSh13Ni"
      },
      "outputs": [],
      "source": [
        "feedback_bench_dataset = load_dataset(\"prometheus-eval/Feedback-Bench\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MtBrFXs90umT",
        "outputId": "ade84757-1e53-4ae8-ef83-b00c89f015e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1000, 16)\n",
            "orig_instruction: I am a project manager for a software development company. We have a new client who wants to develop\n",
            "orig_score3_description: The model frequently provides relevant responses and sometimes asks clarifying questions, maintainin\n",
            "orig_score4_description: The model consistently provides relevant responses and often asks clarifying questions, ensuring a s\n",
            "output: The response does touch upon relevant points such as discussing with the client, asking questions, a\n",
            "orig_response: To understand the client's needs better, maybe try talking to them about what they want. It might he\n",
            "orig_reference_answer: In order to manage this situation effectively and ensure the successful development of the web appli\n",
            "orig_feedback: The response does touch upon relevant points such as discussing with the client, asking questions, a\n",
            "orig_score1_description: The model consistently provides irrelevant responses and fails to ask clarifying questions, leading \n",
            "orig_score: 2\n",
            "orig_criteria: Does the model effectively manage the flow of the conversation, asking clarifying questions when nec\n",
            "orig_score2_description: The model occasionally provides relevant responses, but rarely asks clarifying questions, resulting \n",
            "instruction: ###Task Description:\n",
            "An instruction (might include an Input inside it), a response to evaluate, a re\n",
            "orig_score5_description: The model expertly manages the flow of conversation, consistently providing relevant responses and p\n",
            "input: \n",
            "messages: [{'content': 'You are a fair judge assistant tasked with providing clear, objective feedback based on specific criteria, ensuring each assessment reflects the absolute standards set for performance.###Task Description:\\nAn instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given.\\n1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general.\\n2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\\n3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\\n4. Please do not generate any other opening, closing, and explanations.\\n\\n###The instruction to evaluate:\\nI am a project manager for a software development company. We have a new client who wants to develop a complex web application for their business. The client has explained their requirements, but they are not very clear on the technical aspects. I am not sure about certain features they want us to implement, and there is potential for misunderstanding. How can I effectively communicate with the client to fully understand their needs and ensure the successful development of the web application?\\n\\n###Response to evaluate:\\nTo understand the client\\'s needs better, maybe try talking to them about what they want. It might help if you explain the features in simple words. You can also ask some questions to make sure you know what they\\'re looking for. After discussing, write down the main points and send it to the client. Keep talking to the client while working on the project. This might help with the web application development.\\n\\nFeedback: The response provided occasionally touches upon relevant aspects such as discussing requirements with the client, asking questions, and maintaining communication during development. However, it lacks depth and detail, with many suggestions coming across as vague and not actionable. Additionally, the response does not proactively offer clarifying questions or emphasize the importance of ensuring a seamless flow of conversation. It also falls short of the expected 21-sentence length. So the overall score is 2.\\n\\n###Reference Answer (Score 5):\\nIn order to manage this situation effectively and ensure the successful development of the web application, it\\'s crucial to maintain a clear line of communication with the client, asking the right questions and providing relevant responses.\\n\\nFirstly, schedule a meeting with the client, either face-to-face or via a video call to discuss their requirements in detail. During this meeting, make sure to focus on the specific features they want to implement in their web application. Even if they are not tech-savvy, try to explain in layman terms so that they can understand.\\n\\nNext, as you talk about each feature, ask clarifying questions to ensure you fully understand their needs. For instance, if they want a feature to manage customer data, ask them what specific information they want to track, how they want to categorize it, and who will have access to this data. Proactively asking these types of questions will help prevent misunderstandings and ensure you are on the same page.\\n\\nWhile discussing, keep the conversation focused and relevant. Avoid using technical jargon that the client may not understand. Instead, use analogies or simple explanations to help them understand the implications of their requirements.\\n\\nAfter the meeting, summarize everything discussed and send it to the client as a written document. This document will serve as a reference for both parties and prevent any future confusion. Ask the client to review and confirm if your understanding aligns with their expectations.\\n\\nLastly, maintain continuous communication with the client throughout the development process. Regularly update them about the progress and seek their feedback. This will ensure that any issues or changes in requirements are addressed promptly, leading to a successful project completion.\\n\\nRemember, the key to effective communication is active listening, relevant responses, and proactive questioning. With these strategies, you should be able to manage the flow of conversation effectively, leading to a seamless interaction and successful web application development.\\n\\n###Score Rubrics:\\n[Does the model effectively manage the flow of the conversation, asking clarifying questions when necessary and providing relevant responses?]\\nScore 1: The model consistently provides irrelevant responses and fails to ask clarifying questions, leading to a disjointed conversation.\\nScore 2: The model occasionally provides relevant responses, but rarely asks clarifying questions, resulting in a somewhat disjointed conversation.\\nScore 3: The model frequently provides relevant responses and sometimes asks clarifying questions, maintaining an average flow of conversation.\\nScore 4: The model consistently provides relevant responses and often asks clarifying questions, ensuring a smooth flow of conversation.\\nScore 5: The model expertly manages the flow of conversation, consistently providing relevant responses and proactively asking clarifying questions when necessary, leading to a seamless interaction.\\n\\n###Feedback: ', 'role': 'user'}, {'content': 'The response does touch upon relevant points such as discussing with the client, asking questions, and maintaining communication throughout the project. However, it is very brief and lacks sufficient detail. There are no specific suggestions on how to manage the conversation or actively seek clarifications. The response is also much shorter than the expected 21-sentence length. So the overall score is 2. [RESULT] 2', 'role': 'assistant'}]\n",
            "__index_level_0__: 28676\n"
          ]
        }
      ],
      "source": [
        "seed = 25\n",
        "random.seed(seed)\n",
        "feedback_bench_dataset = feedback_bench_dataset['train'].shuffle(seed=seed)\n",
        "example = feedback_bench_dataset[0]\n",
        "print(feedback_bench_dataset.shape)\n",
        "for key, value in example.items():\n",
        "    print(f\"{key}: {value[:100] if isinstance(value, str) else value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNlsby2ANShY"
      },
      "source": [
        "## 1.2 Summary and Statistical Analysis of Dataset (3 points)\n",
        "In this section, your task is to explore and analyze the dataset both quantitatively and qualitatively.\n",
        "\n",
        "* Describe what the column represents.\n",
        "\n",
        "* Identify columns with integer or numerical values.\n",
        "\n",
        "* Plot the distribution of these columns using histograms or other appropriate visualizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VemOt-wk1xQ_"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8-7rT39Ml7j"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9fDj-EsjCGR"
      },
      "source": [
        "## 1.3 Load Phi-3-3.8B model\n",
        "\n",
        "Use the Hugging Face transformers library to load the model and tokenizer:\n",
        "\n",
        "Model: https://huggingface.co/microsoft/Phi-3-mini-4k-instruct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoyLrVsvjBXw"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftFFnhmKq9Uy"
      },
      "source": [
        "## 1.4 Phi Judgemnt Performance Evaluation (23 points)\n",
        "\n",
        "In this part of the assignment, you will assess the ability of the Phi-3-mini model to generate evaluative judgments based on structured prompts derived from the dataset. Follow the steps below to carry out the inference process and evaluate the model’s performance:\n",
        "\n",
        "**1. Prompt Construction:**\n",
        "\n",
        "\n",
        "Use relevant columns from the dataset (e.g., orig_instruction,orig_criteria, etc.) to construct informative prompts that the model can respond to meaningfully.\n",
        "\n",
        "\n",
        "**2. Model Inference:**\n",
        "\n",
        "Select a random sample of 50 entries from the dataset. For each entry, feed the constructed prompt into the Phi model and generate a corresponding judgment and score.\n",
        "\n",
        "*Don't forget applying chat template 😊*\n",
        "\n",
        "**3. Output Parsing:**\n",
        "\n",
        "After generating model outputs, create a method to extract the predicted score  from the model’s response.\n",
        "\n",
        "\n",
        "**4. Metric Selection and Performance Analysis:**\n",
        "\n",
        "Compare the predicted scores obtained from the model with the original human-annotated scores available in the `orig_score` column of the dataset. This step will help you measure how well the model’s outputs align with refrence judge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2FjGnKoVzcr"
      },
      "source": [
        "### 1.4.1 Prompt Construction (2 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSfK0miRWF9s"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VarMRNQZWG5Q"
      },
      "source": [
        "### 1.4.2 Model Inference (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uysvNUNrqbd8"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKPkfJHqyzQ0"
      },
      "source": [
        "### 1.4.3 Extract Score (Output Parsing) (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3z3INKCWyydu"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GIbqfSO4r4q"
      },
      "source": [
        "### 1.4.4 Metric Selection and Performance Analysis (11 points)\n",
        "\n",
        "Respond to the following questions to deepen your understanding of evaluation strategies in LLM-based scoring tasks:\n",
        "\n",
        "\n",
        "What is the most appropriate evaluation metric for comparing the model’s predicted scores with the reference value (`orig_score`)? Consider the type of scores (e.g., continuous, ordinal, or categorical) when making your choice. (3 points)\n",
        "\n",
        "Calculate the chosen evaluation metric (any suitable metric) to quantify the relationship between the model's predicted score and `orig_score` (6 points).\n",
        "\n",
        "Is accuracy a suitable metric in this context? Why or why not? (2 points)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M92jh31lNd41"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKgnROsi56-z"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e55bgsxHNed8"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFnLvKIR8mmI"
      },
      "source": [
        "## 1.5 Alternative Evaluation Strategies (15 points)\n",
        "\n",
        "In addition to the default scoring approach, you are encouraged to explore alternative judgment strategies to evaluate the model’s performance on the judgment task.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Examples of Alternative Approaches\n",
        "\n",
        "#### Quantetive Prompt Design\n",
        "- Reformulate the prompts to request a **score on a different scale**, such as from **1 to 100** instead of 1 to 5.\n",
        "- After model inference, **normalize** or **map** the predicted score back to the **1–5 range** for comparison (e.g., using simple scaling or binning).\n",
        "\n",
        "#### Qualitative Scoring (Likert-style)\n",
        "- Design prompts to elicit **descriptive judgments**, such as:  \n",
        "  `\"Poor\"`, `\"Fair\"`, `\"Good\"`, `\"Very Good\"`, `\"Excellent\"`\n",
        "- Then **map these qualitative outputs** to **numerical values** (e.g., 1 to 5) to enable metric-based evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI7bUlm-UO0z"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7qDS0cSqRgp"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OQgN6N0qRUZ"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0vc3MJFqUMI"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eE91AL2PqZfv"
      },
      "source": [
        "\n",
        "# 🧩 Part 2: Creating Preference Data Using LLM as Judge\n",
        "\n",
        "In this part, you will explore how to use large language models (LLMs) to generate **preference data** for optimization tasks.\n",
        "\n",
        "We will compare two models:\n",
        "\n",
        "- `Qwen/Qwen1.5-1.8B-Chat`\n",
        "- `stabilityai/stablelm-2-zephyr-1_6b`\n",
        "\n",
        "The goal is to evaluate how well these models can **distinguish preferred answers (\"chosen\") from less favorable ones (\"rejected\")** in a human-like manner.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Trlh0ydZjhz"
      },
      "source": [
        "## 2.1 Download the Models and Dataset\n",
        "\n",
        "- Load the following two models from Hugging Face:\n",
        "  - `Qwen/Qwen1.5-1.8B-Chat`\n",
        "  - `stabilityai/stablelm-2-zephyr-1_6b`\n",
        "\n",
        "- Download the dataset:  \n",
        "  [`HumanLLMs/Human-Like-DPO-Dataset`](https://huggingface.co/datasets/HumanLLMs/Human-Like-DPO-Dataset)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InUGnZdfJL8g"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1S_traP8I-Mq"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4S11PbRZ0Ua"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kohpa-vOHUq"
      },
      "source": [
        "## 2.2 Dataset Exploration (1 point)\n",
        "\n",
        "\n",
        "- Analyze the `HumanLLMs/Human-Like-DPO-Dataset`.\n",
        "  - Describe the dataset structure and columns.\n",
        "\n",
        "- **Optional**: Read the paper for additional context and insights:  \n",
        "   [Human-Like DPO (arXiv:2501.05032)](https://arxiv.org/pdf/2501.05032)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0EkXgwiNz3Z"
      },
      "source": [
        "`# WRITE YOUR ANSWER HERE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4e0fakr9Orq"
      },
      "source": [
        "## 2.3 Judging Setup (3 points)\n",
        "\n",
        "- Create a **prompting framework** that presents both the **chosen** and **rejected** answers to the model and asks it to **select the better one**.\n",
        "\n",
        "\n",
        "Example prompt structure:\n",
        "> \"Here is a prompt and two responses. Please choose the better response based on helpfulness, relevance, and coherence.  \n",
        ">  \n",
        "> Prompt: {prompt}  \n",
        ">  \n",
        "> Response 1: {chosen or rejected}  \n",
        "> Response 2: {rejected or chosen}  \n",
        ">  \n",
        "> Which response is better? Reply with 'Answer 1' or 'Answer 2'.\"\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fykTRxZzcFeD"
      },
      "outputs": [],
      "source": [
        "## Write Your Code Here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0lpkq0NPbaW"
      },
      "source": [
        "## 2.4 Model Comparison (10 points)\n",
        "\n",
        "- Run inference using both models on a **sample of the dataset** (e.g., 200–500 instances from dataset). (2 points)\n",
        "- Compare each model's judgments to the **ground truth** (i.e., whether it preferred the \"chosen\" response). (4 points)\n",
        "- Compute the **accuracy** and plot **confusion matrix** for each model to evaluate performance. (4 points)\n",
        "- Make sure to properly handle cases where the model's output is unclear or the preference cannot be extracted (e.g., skip or categorize as \"unkowned\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyK2-BsiJtnD"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnZM43PMqwD8"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrisL91j9OEs"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKvtiLs3nrrl"
      },
      "outputs": [],
      "source": [
        "# Write Your Code Here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
